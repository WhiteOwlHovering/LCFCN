{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haven import haven_chk as hc\n",
    "from haven import haven_results as hr\n",
    "from haven import haven_utils as hu\n",
    "import torch\n",
    "import torchvision\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import itertools\n",
    "import os\n",
    "import pylab as plt\n",
    "import exp_configs\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from src import models\n",
    "from src import datasets\n",
    "\n",
    "\n",
    "import argparse\n",
    "\n",
    "from torch.utils.data import sampler\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "from torch.backends import cudnn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainval(exp_dict, savedir_base, datadir, reset=False, num_workers=0):\n",
    "    # bookkeepting stuff\n",
    "    # ==================\n",
    "    pprint.pprint(exp_dict)\n",
    "    exp_id = hu.hash_dict(exp_dict)\n",
    "    savedir = os.path.join(savedir_base, exp_id)\n",
    "    if reset:\n",
    "        hc.delete_and_backup_experiment(savedir)\n",
    "\n",
    "    os.makedirs(savedir, exist_ok=True)\n",
    "    hu.save_json(os.path.join(savedir, \"exp_dict.json\"), exp_dict)\n",
    "    print(\"Experiment saved in %s\" % savedir)\n",
    "\n",
    "    # Dataset\n",
    "    # ==================\n",
    "    # train set\n",
    "    train_set = datasets.get_dataset(dataset_dict=exp_dict[\"dataset\"],\n",
    "                                     split=\"train\",\n",
    "                                     datadir=datadir,\n",
    "                                     exp_dict=exp_dict,\n",
    "                                     dataset_size=exp_dict['dataset_size'])\n",
    "    # val set\n",
    "    val_set = datasets.get_dataset(dataset_dict=exp_dict[\"dataset\"],\n",
    "                                   split=\"val\",\n",
    "                                   datadir=datadir,\n",
    "                                   exp_dict=exp_dict,\n",
    "                                   dataset_size=exp_dict['dataset_size'])\n",
    "\n",
    "    val_sampler = torch.utils.data.SequentialSampler(val_set)\n",
    "    val_loader = DataLoader(val_set,\n",
    "                            sampler=val_sampler,\n",
    "                            batch_size=1,\n",
    "                            num_workers=num_workers)\n",
    "    # Model\n",
    "    # ==================\n",
    "    model = models.get_model(model_dict=exp_dict['model'],\n",
    "                             exp_dict=exp_dict,\n",
    "                             train_set=train_set).cuda()\n",
    "\n",
    "    # model.opt = optimizers.get_optim(exp_dict['opt'], model)\n",
    "    model_path = os.path.join(savedir, \"model.pth\")\n",
    "    score_list_path = os.path.join(savedir, \"score_list.pkl\")\n",
    "\n",
    "    if os.path.exists(score_list_path):\n",
    "        # resume experiment\n",
    "        model.load_state_dict(hu.torch_load(model_path))\n",
    "        score_list = hu.load_pkl(score_list_path)\n",
    "        s_epoch = score_list[-1]['epoch'] + 1\n",
    "    else:\n",
    "        # restart experiment\n",
    "        score_list = []\n",
    "        s_epoch = 0\n",
    "\n",
    "    # Train & Val\n",
    "    # ==================\n",
    "    print(\"Starting experiment at epoch %d\" % (s_epoch))\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(\n",
    "        train_set, replacement=True, num_samples=2*len(val_set))\n",
    "\n",
    "    train_loader = DataLoader(train_set,\n",
    "                              sampler=train_sampler,\n",
    "                              batch_size=exp_dict[\"batch_size\"], \n",
    "                              drop_last=True, num_workers=num_workers)\n",
    "\n",
    "    for e in range(s_epoch, exp_dict['max_epoch']):\n",
    "        # Validate only at the start of each cycle\n",
    "        score_dict = {}\n",
    "\n",
    "        # Train the model\n",
    "        train_dict = model.train_on_loader(train_loader)\n",
    "\n",
    "        # Validate and Visualize the model\n",
    "        val_dict = model.val_on_loader(val_loader, \n",
    "                        savedir_images=os.path.join(savedir, \"images\"),\n",
    "                        n_images=3)\n",
    "        score_dict.update(val_dict)\n",
    "        # model.vis_on_loader(\n",
    "        #     vis_loader, savedir=os.path.join(savedir, \"images\"))\n",
    "\n",
    "        # Get new score_dict\n",
    "        score_dict.update(train_dict)\n",
    "        score_dict[\"epoch\"] = len(score_list)\n",
    "\n",
    "        # Add to score_list and save checkpoint\n",
    "        score_list += [score_dict]\n",
    "\n",
    "        # Report & Save\n",
    "        score_df = pd.DataFrame(score_list)\n",
    "        print(\"\\n\", score_df.tail(), \"\\n\")\n",
    "        hu.torch_save(model_path, model.get_state_dict())\n",
    "        hu.save_pkl(score_list_path, score_list)\n",
    "        print(\"Checkpoint Saved: %s\" % savedir)\n",
    "\n",
    "        # Save Best Checkpoint\n",
    "        if e == 0 or (score_dict.get(\"val_score\", 0) > score_df[\"val_score\"][:-1].fillna(0).max()):\n",
    "            hu.save_pkl(os.path.join(\n",
    "                savedir, \"score_list_best.pkl\"), score_list)\n",
    "            hu.torch_save(os.path.join(savedir, \"model_best.pth\"),\n",
    "                          model.get_state_dict())\n",
    "            print(\"Saved Best: %s\" % savedir)\n",
    "\n",
    "    print('Experiment completed et epoch %d' % e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "[{'dataset': {'name': 'virtualOR', 'transform': 'rgb_normalize'}, 'model': {'name': 'lcfcn', 'base': 'fcn8_vgg16'}, 'batch_size': 1, 'max_epoch': 1, 'dataset_size': {'train': 'all', 'val': 'all'}, 'optimizer': 'adam', 'lr': 1e-05}]\n",
      "running alt\n",
      "{'batch_size': 1,\n",
      " 'dataset': {'name': 'virtualOR', 'transform': 'rgb_normalize'},\n",
      " 'dataset_size': {'train': 'all', 'val': 'all'},\n",
      " 'lr': 1e-05,\n",
      " 'max_epoch': 1,\n",
      " 'model': {'base': 'fcn8_vgg16', 'name': 'lcfcn'},\n",
      " 'optimizer': 'adam'}\n",
      "Experiment saved in ../../../../media/James/BigData/LCFCN_temp_folder/f8f5c56297038020cd7d7f8b9035d31d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/840 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment at epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training. Loss: 85.9867:   0%|          | 2/840 [00:10<1:28:48,  6.36s/it]"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\"batch_size\": [1,5,10],\n",
    "         \"max_epoch\": [100],\n",
    "'''\n",
    "    \n",
    "    \n",
    "exp_group_list = ['virtualOR']\n",
    "savedir_base = '../../../../media/James/BigData/LCFCN_temp_folder'\n",
    "datadir = '../../../../media/James/BigData/TRANCOS_v3'\n",
    "reset = 1\n",
    "exp_id = None\n",
    "run_jobs = False\n",
    "num_workers = 0\n",
    "\n",
    "\n",
    "# Collect experiments\n",
    "# ===================\n",
    "if exp_id is not None:\n",
    "    # select one experiment\n",
    "    savedir = os.path.join(savedir_base, args.exp_id)\n",
    "    exp_dict = hu.load_json(os.path.join(savedir, \"exp_dict.json\"))\n",
    "\n",
    "    exp_list = [exp_dict]\n",
    "\n",
    "else:\n",
    "    # select exp group\n",
    "    print('here')\n",
    "    exp_list = []\n",
    "    for exp_group_name in exp_group_list:\n",
    "        exp_list += exp_configs.EXP_GROUPS[exp_group_name]\n",
    "        \n",
    "    print(exp_list)\n",
    "\n",
    "# Run experiments\n",
    "# ===============\n",
    "if run_jobs:\n",
    "    print('running')\n",
    "    from haven import haven_jobs as hjb\n",
    "    jm = hjb.JobManager(exp_list=exp_list, savedir_base=savedir_base)\n",
    "    jm_summary_list = jm.get_summary()\n",
    "    #print(jm.get_summary()['status'])\n",
    " \n",
    "    import usr_configs as uc\n",
    "    uc.run_jobs(exp_list, savedir_base, datadir)\n",
    "\n",
    "else:\n",
    "    print('running alt')\n",
    "    for exp_dict in exp_list:\n",
    "        # do trainval\n",
    "        trainval(exp_dict=exp_dict,\n",
    "                savedir_base=savedir_base,\n",
    "                datadir=datadir,\n",
    "                reset=reset,\n",
    "                num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from haven import haven_jupyter as hj\n",
    "# from haven import haven_results as hr\n",
    "\n",
    "# # path to where the experiments got saved\n",
    "# savedir_base = '../../../../media/James/BigData/LCFCN_temp_folder/'\n",
    "\n",
    "# # filter exps\n",
    "# filterby_list = [('dataset.name','trancos')]\n",
    "# # get experiments\n",
    "# rm = hr.ResultManager(savedir_base=savedir_base )\n",
    "# # rm = hr.ResultManager(savedir_base=savedir_base, filterby_list=filterby_list, verbose=0)\n",
    "# # dashboard variables\n",
    "# legend_list = ['model.base']\n",
    "# title_list = ['dataset', 'model']\n",
    "# y_metrics = ['val_mae']\n",
    "\n",
    "# # launch dashboard\n",
    "# hj.get_dashboard(rm, vars(), show_jobs=False,wide_display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
